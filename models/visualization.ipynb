{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "#from cs231n.image_utils import SQUEEZENET_MEAN, SQUEEZENET_STD\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from skimage import io, transform, color\n",
    "import os\n",
    "import math\n",
    "import torchvision.models as models\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/'\n",
    "size = 193\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [0.49439774802337344,0.5177020917650417,0.5013496452715945]\n",
    "stds = [0.16087996922691195,0.15714445907773483,0.1605951051365687]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.21581421730357, 6.363571492554687, 6.226839847638]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1/std for std in stds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, size=size,means=means,stds=stds):\n",
    "    transform = T.Compose([\n",
    "        T.Resize(size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=means,\n",
    "                    std=stds),\n",
    "        T.Lambda(lambda x: x[None]),\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def deprocess(img, should_rescale=True):\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda x: x[0]),\n",
    "        T.Normalize(mean=[0, 0, 0], std=[1/std for std in stds]),\n",
    "        T.Normalize(mean=-means, std=[1, 1, 1]),\n",
    "        T.Lambda(rescale) if should_rescale else T.Lambda(lambda x: x),\n",
    "        T.ToPILImage(),\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def rescale(x):\n",
    "    low, high = x.min(), x.max()\n",
    "    x_rescaled = (x - low) / (high - low)\n",
    "    return x_rescaled\n",
    "    \n",
    "def blur_image(X, sigma=1):\n",
    "    X_np = X.cpu().clone().numpy()\n",
    "    X_np = gaussian_filter1d(X_np, sigma, axis=2)\n",
    "    X_np = gaussian_filter1d(X_np, sigma, axis=3)\n",
    "    X.copy_(torch.Tensor(X_np).type_as(X))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('classification_checkpoint.pth')['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_labels = pd.read_csv('../data_description/A1_A2_C1_filtered_validation_v2.csv')\n",
    "validation_image_names = {2:validation_labels['image_name_2'],8:validation_labels['image_name_8'],5:validation_labels['image_name_5']}\n",
    "validation_y = validation_labels['has_cell_13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0      well_A2/well3362_day02_well.png\n",
       " 1      well_A1/well2413_day02_well.png\n",
       " 2      well_C1/well2126_day02_well.png\n",
       " 3      well_C1/well0543_day02_well.png\n",
       " 4      well_A1/well0184_day02_well.png\n",
       " 5      well_C1/well2965_day02_well.png\n",
       " 6      well_A2/well0203_day02_well.png\n",
       " 7      well_A1/well4708_day02_well.png\n",
       " 8      well_A1/well1842_day02_well.png\n",
       " 9      well_A2/well2752_day02_well.png\n",
       " 10     well_A1/well1484_day02_well.png\n",
       " 11     well_C1/well2141_day02_well.png\n",
       " 12     well_C1/well3198_day02_well.png\n",
       " 13     well_A1/well0968_day02_well.png\n",
       " 14     well_A1/well1172_day02_well.png\n",
       " 15     well_A1/well1745_day02_well.png\n",
       " 16     well_A1/well2637_day02_well.png\n",
       " 17     well_A1/well2436_day02_well.png\n",
       " 18     well_A1/well2514_day02_well.png\n",
       " 19     well_C1/well0010_day02_well.png\n",
       " 20     well_C1/well1652_day02_well.png\n",
       " 21     well_A2/well0123_day02_well.png\n",
       " 22     well_C1/well2712_day02_well.png\n",
       " 23     well_A1/well0957_day02_well.png\n",
       " 24     well_C1/well0946_day02_well.png\n",
       " 25     well_C1/well2201_day02_well.png\n",
       " 26     well_A2/well2101_day02_well.png\n",
       " 27     well_A2/well0009_day02_well.png\n",
       " 28     well_C1/well0107_day02_well.png\n",
       " 29     well_A1/well3455_day02_well.png\n",
       "                     ...               \n",
       " 784    well_A2/well0162_day02_well.png\n",
       " 785    well_A1/well2784_day02_well.png\n",
       " 786    well_A1/well2584_day02_well.png\n",
       " 787    well_A1/well0751_day02_well.png\n",
       " 788    well_A1/well1398_day02_well.png\n",
       " 789    well_A2/well0288_day02_well.png\n",
       " 790    well_C1/well1157_day02_well.png\n",
       " 791    well_A2/well1441_day02_well.png\n",
       " 792    well_A1/well2903_day02_well.png\n",
       " 793    well_C1/well3061_day02_well.png\n",
       " 794    well_C1/well3145_day02_well.png\n",
       " 795    well_A1/well1022_day02_well.png\n",
       " 796    well_C1/well1768_day02_well.png\n",
       " 797    well_C1/well2714_day02_well.png\n",
       " 798    well_A1/well2969_day02_well.png\n",
       " 799    well_A2/well3373_day02_well.png\n",
       " 800    well_A1/well0828_day02_well.png\n",
       " 801    well_A2/well1501_day02_well.png\n",
       " 802    well_A1/well3568_day02_well.png\n",
       " 803    well_C1/well2656_day02_well.png\n",
       " 804    well_A1/well2029_day02_well.png\n",
       " 805    well_A1/well4354_day02_well.png\n",
       " 806    well_C1/well0431_day02_well.png\n",
       " 807    well_C1/well1415_day02_well.png\n",
       " 808    well_A1/well0070_day02_well.png\n",
       " 809    well_A1/well1021_day02_well.png\n",
       " 810    well_C1/well1885_day02_well.png\n",
       " 811    well_A2/well3438_day02_well.png\n",
       " 812    well_C1/well0581_day02_well.png\n",
       " 813    well_A1/well1625_day02_well.png\n",
       " Name: image_name_2, Length: 814, dtype: object,\n",
       " 8: 0      well_A2/well3362_day08_well.png\n",
       " 1      well_A1/well2413_day08_well.png\n",
       " 2      well_C1/well2126_day08_well.png\n",
       " 3      well_C1/well0543_day08_well.png\n",
       " 4      well_A1/well0184_day08_well.png\n",
       " 5      well_C1/well2965_day08_well.png\n",
       " 6      well_A2/well0203_day08_well.png\n",
       " 7      well_A1/well4708_day08_well.png\n",
       " 8      well_A1/well1842_day08_well.png\n",
       " 9      well_A2/well2752_day08_well.png\n",
       " 10     well_A1/well1484_day08_well.png\n",
       " 11     well_C1/well2141_day08_well.png\n",
       " 12     well_C1/well3198_day08_well.png\n",
       " 13     well_A1/well0968_day08_well.png\n",
       " 14     well_A1/well1172_day08_well.png\n",
       " 15     well_A1/well1745_day08_well.png\n",
       " 16     well_A1/well2637_day08_well.png\n",
       " 17     well_A1/well2436_day08_well.png\n",
       " 18     well_A1/well2514_day08_well.png\n",
       " 19     well_C1/well0010_day08_well.png\n",
       " 20     well_C1/well1652_day08_well.png\n",
       " 21     well_A2/well0123_day08_well.png\n",
       " 22     well_C1/well2712_day08_well.png\n",
       " 23     well_A1/well0957_day08_well.png\n",
       " 24     well_C1/well0946_day08_well.png\n",
       " 25     well_C1/well2201_day08_well.png\n",
       " 26     well_A2/well2101_day08_well.png\n",
       " 27     well_A2/well0009_day08_well.png\n",
       " 28     well_C1/well0107_day08_well.png\n",
       " 29     well_A1/well3455_day08_well.png\n",
       "                     ...               \n",
       " 784    well_A2/well0162_day08_well.png\n",
       " 785    well_A1/well2784_day08_well.png\n",
       " 786    well_A1/well2584_day08_well.png\n",
       " 787    well_A1/well0751_day08_well.png\n",
       " 788    well_A1/well1398_day08_well.png\n",
       " 789    well_A2/well0288_day08_well.png\n",
       " 790    well_C1/well1157_day08_well.png\n",
       " 791    well_A2/well1441_day08_well.png\n",
       " 792    well_A1/well2903_day08_well.png\n",
       " 793    well_C1/well3061_day08_well.png\n",
       " 794    well_C1/well3145_day08_well.png\n",
       " 795    well_A1/well1022_day08_well.png\n",
       " 796    well_C1/well1768_day08_well.png\n",
       " 797    well_C1/well2714_day08_well.png\n",
       " 798    well_A1/well2969_day08_well.png\n",
       " 799    well_A2/well3373_day08_well.png\n",
       " 800    well_A1/well0828_day08_well.png\n",
       " 801    well_A2/well1501_day08_well.png\n",
       " 802    well_A1/well3568_day08_well.png\n",
       " 803    well_C1/well2656_day08_well.png\n",
       " 804    well_A1/well2029_day08_well.png\n",
       " 805    well_A1/well4354_day08_well.png\n",
       " 806    well_C1/well0431_day08_well.png\n",
       " 807    well_C1/well1415_day08_well.png\n",
       " 808    well_A1/well0070_day08_well.png\n",
       " 809    well_A1/well1021_day08_well.png\n",
       " 810    well_C1/well1885_day08_well.png\n",
       " 811    well_A2/well3438_day08_well.png\n",
       " 812    well_C1/well0581_day08_well.png\n",
       " 813    well_A1/well1625_day08_well.png\n",
       " Name: image_name_8, Length: 814, dtype: object,\n",
       " 5: 0      well_A2/well3362_day05_well.png\n",
       " 1      well_A1/well2413_day05_well.png\n",
       " 2      well_C1/well2126_day05_well.png\n",
       " 3      well_C1/well0543_day05_well.png\n",
       " 4      well_A1/well0184_day05_well.png\n",
       " 5      well_C1/well2965_day05_well.png\n",
       " 6      well_A2/well0203_day05_well.png\n",
       " 7      well_A1/well4708_day05_well.png\n",
       " 8      well_A1/well1842_day05_well.png\n",
       " 9      well_A2/well2752_day05_well.png\n",
       " 10     well_A1/well1484_day05_well.png\n",
       " 11     well_C1/well2141_day05_well.png\n",
       " 12     well_C1/well3198_day05_well.png\n",
       " 13     well_A1/well0968_day05_well.png\n",
       " 14     well_A1/well1172_day05_well.png\n",
       " 15     well_A1/well1745_day05_well.png\n",
       " 16     well_A1/well2637_day05_well.png\n",
       " 17     well_A1/well2436_day05_well.png\n",
       " 18     well_A1/well2514_day05_well.png\n",
       " 19     well_C1/well0010_day05_well.png\n",
       " 20     well_C1/well1652_day05_well.png\n",
       " 21     well_A2/well0123_day05_well.png\n",
       " 22     well_C1/well2712_day05_well.png\n",
       " 23     well_A1/well0957_day05_well.png\n",
       " 24     well_C1/well0946_day05_well.png\n",
       " 25     well_C1/well2201_day05_well.png\n",
       " 26     well_A2/well2101_day05_well.png\n",
       " 27     well_A2/well0009_day05_well.png\n",
       " 28     well_C1/well0107_day05_well.png\n",
       " 29     well_A1/well3455_day05_well.png\n",
       "                     ...               \n",
       " 784    well_A2/well0162_day05_well.png\n",
       " 785    well_A1/well2784_day05_well.png\n",
       " 786    well_A1/well2584_day05_well.png\n",
       " 787    well_A1/well0751_day05_well.png\n",
       " 788    well_A1/well1398_day05_well.png\n",
       " 789    well_A2/well0288_day05_well.png\n",
       " 790    well_C1/well1157_day05_well.png\n",
       " 791    well_A2/well1441_day05_well.png\n",
       " 792    well_A1/well2903_day05_well.png\n",
       " 793    well_C1/well3061_day05_well.png\n",
       " 794    well_C1/well3145_day05_well.png\n",
       " 795    well_A1/well1022_day05_well.png\n",
       " 796    well_C1/well1768_day05_well.png\n",
       " 797    well_C1/well2714_day05_well.png\n",
       " 798    well_A1/well2969_day05_well.png\n",
       " 799    well_A2/well3373_day05_well.png\n",
       " 800    well_A1/well0828_day05_well.png\n",
       " 801    well_A2/well1501_day05_well.png\n",
       " 802    well_A1/well3568_day05_well.png\n",
       " 803    well_C1/well2656_day05_well.png\n",
       " 804    well_A1/well2029_day05_well.png\n",
       " 805    well_A1/well4354_day05_well.png\n",
       " 806    well_C1/well0431_day05_well.png\n",
       " 807    well_C1/well1415_day05_well.png\n",
       " 808    well_A1/well0070_day05_well.png\n",
       " 809    well_A1/well1021_day05_well.png\n",
       " 810    well_C1/well1885_day05_well.png\n",
       " 811    well_A2/well3438_day05_well.png\n",
       " 812    well_C1/well0581_day05_well.png\n",
       " 813    well_A1/well1625_day05_well.png\n",
       " Name: image_name_5, Length: 814, dtype: object}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_image_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXimage(index):\n",
    "    all_images_list = []\n",
    "    for day,img_names in validation_image_names.items():\n",
    "            #print(day, \"   \", index)\n",
    "            \n",
    "        img_name = img_names[index]\n",
    "        img_loc = os.path.join(path, img_name)\n",
    "        image = io.imread(img_loc)\n",
    "#         mean, sd = mean_sd_dict[day]\n",
    "#         image = np.true_divide(color.rgb2gray(image) - mean, sd)\n",
    "        image = color.rgb2gray(image)\n",
    "        all_images_list.append(image)\n",
    "    images = np.array(all_images_list)\n",
    "    #images = np.reshape(images, (1,2,0))\n",
    "    #return torch.from_numpy(images).float()\n",
    "    return images\n",
    "def getY(index):\n",
    "    Y = validation_y[index]\n",
    "    #return torch.from_numpy(np.asarray(validation_y[index], dtype=float)).float()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXimage(index):\n",
    "    all_images_list = []\n",
    "    for day,img_names in validation_image_names.items():\n",
    "            #print(day, \"   \", index)\n",
    "            \n",
    "        img_name = img_names[index]\n",
    "        img_loc = os.path.join(path, img_name)\n",
    "        image = io.imread(img_loc)\n",
    "#         mean, sd = mean_sd_dict[day]\n",
    "#         image = np.true_divide(color.rgb2gray(image) - mean, sd)\n",
    "        image = color.rgb2gray(image)\n",
    "        all_images_list.append(image)\n",
    "    images = np.array(all_images_list)\n",
    "    #images = np.reshape(images, (1,2,0))\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = getXimage(0)\n",
    "y = getY(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for i in range(5):\n",
    "    X.append(getXimage(i))\n",
    "    Y.append(getY(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(np.array(X)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y = torch.from_numpy(np.array(Y)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    0\n",
       "3    0\n",
       "4    1\n",
       "Name: has_cell_13, dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.LongTensor(validation_y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 193, 193])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2489\u001b[0m             \u001b[0mtypekey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'typestr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2490\u001b[0;31m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2491\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 193), '<f8')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-3f21982a8b75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2490\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2492\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot handle this data type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2493\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2494\u001b[0m         \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type"
     ]
    }
   ],
   "source": [
    "preprocess(Image.fromarray(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saliency Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1067,  1.0481,  1.1629,  0.7058,  1.3044],\n",
      "        [-0.3272, -0.2228,  1.1700,  1.9523, -0.5114],\n",
      "        [ 0.7604, -0.1452,  0.2288, -0.4644,  1.1930],\n",
      "        [ 0.8360, -0.3876, -0.2788,  1.5241, -1.7641]])\n",
      "tensor([1, 2, 1, 3])\n",
      "tensor([ 1.0481,  1.1700, -0.1452,  1.5241])\n"
     ]
    }
   ],
   "source": [
    "# Example of using gather to select one entry from each row in PyTorch\n",
    "def gather_example():\n",
    "    N, C = 4, 5\n",
    "    s = torch.randn(N, C)\n",
    "    y = torch.LongTensor([1, 2, 1, 3])\n",
    "    print(s)\n",
    "    print(y)\n",
    "    print(s.gather(1, y.view(-1, 1)).squeeze())\n",
    "gather_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    N, C = 4, 5\n",
    "    s = torch.randn(N, C)\n",
    "    y = torch.LongTensor([1, 2, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view(-1,1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-114-91807e6936a7>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-114-91807e6936a7>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    correct_scores..to(device=device).backward(torch.ones(N))\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def compute_saliency_maps(X, y, model):\n",
    "    \"\"\"\n",
    "    Compute a class saliency map using the model for images X and labels y.\n",
    "\n",
    "    Input:\n",
    "    - X: Input images; Tensor of shape (N, 3, H, W)\n",
    "    - y: Labels for X; LongTensor of shape (N,)\n",
    "    - model: A pretrained CNN that will be used to compute the saliency map.\n",
    "\n",
    "    Returns:\n",
    "    - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input\n",
    "    images.\n",
    "    \"\"\"\n",
    "    # Make sure the model is in \"test\" mode\n",
    "    model.eval()\n",
    "    X = X.to(device=device, dtype=dtype) \n",
    "    y = y.to(device=device)\n",
    "    # Make input tensor require gradient\n",
    "    X.requires_grad_()\n",
    "    \n",
    "    saliency = None\n",
    "    N = y.shape[0]\n",
    "    \n",
    "    scores = model(X)\n",
    "    correct_scores = scores.gather(1,y.view(-1,1)).squeeze()\n",
    "    correct_scores..to(device=device).backward(torch.ones(N))\n",
    "    \n",
    "    saliency = X.grad.data.abs()\n",
    "    saliency,_=torch.max(saliency,dim=1)\n",
    "    saliency = saliency.squeeze()\n",
    "\n",
    "    return saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid gradient at index 0 - expected type torch.cuda.FloatTensor but got torch.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-435947a635d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msaliency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_saliency_maps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-112-f987ec370e54>\u001b[0m in \u001b[0;36mcompute_saliency_maps\u001b[0;34m(X, y, model)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mcorrect_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mcorrect_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0msaliency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid gradient at index 0 - expected type torch.cuda.FloatTensor but got torch.FloatTensor"
     ]
    }
   ],
   "source": [
    "X_tensor = X\n",
    "y_tensor = y\n",
    "saliency = compute_saliency_maps(X_tensor, y_tensor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
